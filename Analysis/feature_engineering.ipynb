{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data into dataframe -- energy prices for houston, north, south, and west load zones, predicted and actual energy load for all four load zones, and oil prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\achra\\OneDrive\\Documents\\GitHub\\ERCOT_Price_Prediction_Stat_413_Final_Project\\Data\\CombinedData2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Oil Price'] = df['Oil Price'].ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Preprocessing before training LSTM Model:\n",
    "\n",
    "    1) Need to engineer lags for all energy price columns, and all actual load columns. Lags will be 1 hour, 24 hours, and 168 hours for price columns, and  24 hours for load columns.\n",
    "    2) Need to create exponential moving average (EMA) mean price and EMA price standard deviation variables for north price, west price, south price, and houston price. \n",
    "    3) Need to generate pairwise differences between north, west, south, and houston prices, but using the lagged version (168 hours ago) as the actual variable\n",
    "    4) Remove first week of data due to lag features being null.\n",
    "    5) Delete predicted load columns, save the last week of predicted load values to replace actual load with for the test set.\n",
    "    6) Encode month cyclically as sin and cos values.\n",
    "    7) Encode day of the week cyclically as sin and cos values.\n",
    "    8) Encode hour cyclically as sin and cos values.\n",
    "    8) Add binary holiday column to indicate whether a price observation was during a holiday or not.\n",
    "    9) Use standard scaler to normalize all numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering Lagged Features\n",
    "\n",
    "def create_lagged_features(df, price_cols_and_lags, load_cols_and_lags):\n",
    "    \"\"\"\n",
    "    Create lagged features for price and load columns.\n",
    "    price_cols_and_lags: dictionary with keys as column names and values as a list of lags\n",
    "    load_cols_and_lags: dictionary with keys as column names and values as a list oflags\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col, lags in price_cols_and_lags.items():\n",
    "        for lag in lags:\n",
    "            new_df[f'{col}_lag{lag}'] = new_df[col].shift(lag)\n",
    "    \n",
    "    for col, lags in load_cols_and_lags.items():\n",
    "        for lag in lags:\n",
    "            new_df[f'{col}_lag{lag}'] = new_df[col].shift(lag)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "price_cols_and_lags = {col: [1,24,168] for col in ['North Price', 'Houston Price', 'South Price', 'West Price']}\n",
    "load_cols_and_lags = {col: [24] for col in ['North Load', 'Houston Load', 'South Load', 'West Load']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ema_mean_and_sd(df, ema_cols, span = 168):\n",
    "    \"\"\"\n",
    "        Create Exponential Moving Average (EMA) Mean and Exponential Moving Average Standard Deviation (SD) for specified columns.\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "        ema_cols (list of str): List of column names for which to calculate the EMA mean and SD.\n",
    "        span (int, optional): The span for the EMA calculation. Default is 168.\n",
    "        Returns:\n",
    "        pandas.DataFrame: A new DataFrame with the original columns and additional columns for EMA mean and SD for each specified column.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for col in ema_cols:\n",
    "        new_df[f'{col}_ema_mean'] = new_df[col].ewm(span= span).mean()\n",
    "        new_df[f'{col}_ema_std'] = new_df[col].ewm(span= span).std()\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "ema_cols = ['North Price', 'Houston Price', 'South Price', 'West Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairwise_differences_and_lags(df, price_cols, lag = 168):\n",
    "    new_df = df.copy()\n",
    "    pairwise_price_combinations = list(itertools.combinations(price_cols, 2))\n",
    "    for price1, price2 in pairwise_price_combinations:\n",
    "        new_df[\"{}_{}_diff\".format(price1, price2)] = new_df[price1] - new_df[price2]\n",
    "        new_df[\"{}_{}_diff_lag{}\".format(price1, price2, lag)] = new_df[\"{}_{}_diff\".format(price1, price2)].shift(lag)\n",
    "        new_df.drop(labels = [\"{}_{}_diff\".format(price1, price2)], axis = 1, inplace = True)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_missing_values(df, rows = 168):\n",
    "    \"\"\"\n",
    "    Delete rows with missing data due to adding lagged features.\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    rows (int, optional): The number of rows to delete. Default is 168.\n",
    "    Returns:\n",
    "    pandas.DataFrame: A new DataFrame with the specified number of rows deleted.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df = new_df.iloc[rows:]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_predicted_load(df, load_cols, predicted_load_cols, test_data_size):\n",
    "    \"\"\"\n",
    "    Delete the predicted load columns, but replace the last test_data_size rows from the actual load values with the corresponding predicted load values.\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    for load_col, predicted_load_col in zip(load_cols, predicted_load_cols):\n",
    "        new_df.iloc[-test_data_size:, new_df.columns.get_loc(load_col)] = new_df.iloc[-test_data_size:,new_df.columns.get_loc(predicted_load_col)]\n",
    "    for predicted_load_col in predicted_load_cols:\n",
    "        new_df.drop(predicted_load_col, axis = 1, inplace = True)\n",
    "    return new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_month(df):\n",
    "    \"\"\"\n",
    "    Encode the month as a cyclic feature\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df['month'] = pd.to_datetime(new_df['Date']).dt.month\n",
    "    new_df['month_sin'] = np.sin(2 * np.pi * new_df['month'] / 12)\n",
    "    new_df['month_cos'] = np.cos(2 * np.pi * new_df['month'] / 12)\n",
    "    new_df.drop(labels = ['month'], axis = 1, inplace = True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_day_of_week(df):\n",
    "    \"\"\"\n",
    "    Encode the day of the week as a cyclic feature\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    new_df['day_of_week'] = pd.to_datetime(new_df['Date']).dt.dayofweek\n",
    "    new_df[\"day_of_week_sin\"] = np.sin(2 * np.pi * new_df[\"day_of_week\"] / 7)\n",
    "    new_df[\"day_of_week_cos\"] = np.cos(2 * np.pi * new_df[\"day_of_week\"] / 7)\n",
    "    new_df.drop(labels = [\"day_of_week\"], axis = 1, inplace = True)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_holidays(df):\n",
    "    \"\"\"\n",
    "    Encode holidays as a binary feature\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    tx_holidays = holidays.US(state = 'TX')\n",
    "    new_df[\"Is_Holiday\"] = new_df[\"Date\"].apply(lambda x : 1 if x in tx_holidays else 0)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_hour(df):\n",
    "    \"\"\"\n",
    "    Encode the hour as a cyclic feature\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    hour = new_df['Time'].str.split(':').str[0].astype(int)\n",
    "    new_df['hour_sin'] = np.sin(2 * np.pi * hour / 24)\n",
    "    new_df['hour_cos'] = np.cos(2 * np.pi * hour / 24)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_cols = ['North Load', 'Houston Load', 'South Load', 'West Load']\n",
    "predicted_load_cols = ['North Predicted Load', 'Houston Predicted Load', 'South Predicted Load', 'West Predicted Load']\n",
    "df_1 = create_lagged_features(df, price_cols_and_lags, load_cols_and_lags)\n",
    "df_2 = create_ema_mean_and_sd(df_1, ema_cols)\n",
    "df_3 = create_pairwise_differences_and_lags(df_2, ['North Price', 'Houston Price', 'South Price', 'West Price'])\n",
    "df_4 = delete_rows_with_missing_values(df_3)\n",
    "df_5 = encode_month(df_4)\n",
    "df_6 = encode_day_of_week(df_5)\n",
    "df_7 = encode_holidays(df_6)\n",
    "df_8 = encode_hour(df_7)\n",
    "df_final = delete_predicted_load(df_8, load_cols, predicted_load_cols, 168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = [col for col in df_final if col not in ['Date', 'Time','month_sin', 'month_cos', 'day_of_week_sin', 'day_of_week_cos', 'Is_Holiday', 'hour_sin', 'hour_cos']]\n",
    "def scale_data(df, columns_to_scale):\n",
    "    \"\"\"\n",
    "    Scale the data using StandardScaler\n",
    "    \"\"\"\n",
    "    new_df = df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    new_df[columns_to_scale] = scaler.fit_transform(new_df[columns_to_scale])\n",
    "    return (new_df, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale_data(df, columns_to_scale, test_size=168):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets, and scale the numerical features.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing the data.\n",
    "    columns_to_scale (list): List of column names to be scaled.\n",
    "    test_size (int, optional): The number of rows to be used as the test set. Default is 168.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the scaled training DataFrame, scaled test DataFrame, and the scaler object.\n",
    "    \"\"\"\n",
    "    df_train = df.iloc[:-test_size]\n",
    "    df_test = df.iloc[-test_size:]\n",
    "    \n",
    "    df_train_scaled, scaler = scale_data(df_train, columns_to_scale)\n",
    "    df_test_scaled = df_test.copy()\n",
    "    df_test_scaled[columns_to_scale] = scaler.transform(df_test[columns_to_scale])\n",
    "    \n",
    "    return df_train_scaled, df_test_scaled, scaler\n",
    "\n",
    "df_train_scaled, df_test_scaled, scaler = split_and_scale_data(df_final, columns_to_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_datetime_features(df_train_scaled, df_test_scaled):\n",
    "   \"\"\"\n",
    "   Separates date and time features from scaled dataframes and returns them along with modified dataframes\n",
    "   \n",
    "   Parameters:\n",
    "   df_train_scaled (pd.DataFrame): Scaled training dataframe with Date and Time columns\n",
    "   df_test_scaled (pd.DataFrame): Scaled test dataframe with Date and Time columns\n",
    "   \n",
    "   Returns:\n",
    "   tuple: (date_train, date_test, time_train, time_test, df_train_scaled, df_test_scaled)\n",
    "   \"\"\"\n",
    "   # Store date and time columns\n",
    "   date_train = df_train_scaled['Date']\n",
    "   date_test = df_test_scaled['Date']\n",
    "   time_train = df_train_scaled['Time'] \n",
    "   time_test = df_test_scaled['Time']\n",
    "   \n",
    "   # Drop date and time columns\n",
    "   df_train_scaled.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "   df_test_scaled.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "   df_train_scaled.ffill(inplace=True)\n",
    "   df_test_scaled.ffill(inplace=True)\n",
    "   \n",
    "   return date_train, date_test, time_train, time_test, df_train_scaled, df_test_scaled\n",
    "\n",
    "date_train, date_test, time_train, time_test, df_train_scaled, df_test_scaled = separate_datetime_features(df_train_scaled, df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the variables to a file\n",
    "with open('train_test_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'date_train': date_train,\n",
    "        'date_test': date_test,\n",
    "        'time_train': time_train,\n",
    "        'time_test': time_test,\n",
    "        'df_train_scaled': df_train_scaled,\n",
    "        'df_test_scaled': df_test_scaled,\n",
    "        'scaler': scaler\n",
    "    }, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
